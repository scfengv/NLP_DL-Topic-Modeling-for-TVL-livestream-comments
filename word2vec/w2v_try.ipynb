{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import jieba\n",
    "import spacy\n",
    "import opencc\n",
    "import string\n",
    "import gensim\n",
    "import requests\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import zh_core_web_trf\n",
    "import spacy_transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from stopwordsiso import stopwords\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.pipeline.textcat_multilabel import Config, multi_label_cnn_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/shenchingfeng/Desktop/Topic Modeling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./chat_df_w_file_usr_time_text.csv')\n",
    "df_emoji = pd.read_csv('./df_emoji.csv', encoding = 'utf-8')\n",
    "df_remove_emoji = pd.read_csv('./df_remove_emoji.csv', encoding = 'utf-8')\n",
    "df_emoji_to_desc = pd.read_csv('./df_emoji_to_desc.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_combined = list(stopwords([\"zh\"]))\n",
    "\n",
    "cc = opencc.OpenCC('s2t.json')\n",
    "stopword = []\n",
    "\n",
    "for i in stop_words_combined:\n",
    "    stopword.append(cc.convert(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/56/1hl93h654354jd5vh2zjmd640000gn/T/jieba.cache\n",
      "Loading model cost 0.458 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('./dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation + \"！？&#8203;``【oaicite:0】``&#8203;（）［］《》、，。；：‘“’”…￥·\")\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return text_without_punct\n",
    "\n",
    "def jieba_cut(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    text_without_punct = remove_punctuation(text)\n",
    "    seg_list = [seg for seg in jieba.cut(text_without_punct) if seg.strip()]\n",
    "    \n",
    "    return seg_list\n",
    "\n",
    "def tokenizer(doc):\n",
    "\n",
    "    seg = jieba_cut(doc)\n",
    "    filtered_seg = [word for word in seg if(word not in stopword) and (len(word) > 1)]\n",
    "\n",
    "    return filtered_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for index, row in df_remove_emoji.iterrows():\n",
    "    segments = tokenizer(row['text'])\n",
    "    final.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./word2vec/df_1209/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./1209_w2v_try.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('聊天室', 0.9174688458442688)\n",
      "('她', 0.9076641798019409)\n",
      "('用', 0.9059139490127563)\n",
      "('不用', 0.9002881050109863)\n",
      "('這樣', 0.8989401459693909)\n",
      "('觸網', 0.89765864610672)\n",
      "('自己', 0.895721971988678)\n",
      "('鄧公', 0.8947786092758179)\n",
      "('只是', 0.8936440348625183)\n",
      "('連董', 0.8910359740257263)\n",
      "('他', 0.884768545627594)\n",
      "('時間', 0.8831663131713867)\n",
      "('主審', 0.883057713508606)\n",
      "('還要', 0.8780483603477478)\n",
      "('不然', 0.876921534538269)\n",
      "('先', 0.8766741752624512)\n",
      "('跑', 0.8761611580848694)\n",
      "('他們', 0.8760174512863159)\n",
      "('結果', 0.8752740621566772)\n",
      "('問題', 0.8745262026786804)\n",
      "('XD', 0.8745228052139282)\n",
      "('直播', 0.874515175819397)\n",
      "('暫停', 0.8738765716552734)\n",
      "('挑戰', 0.8737232685089111)\n",
      "('一直', 0.8729910850524902)\n",
      "('哪', 0.8714717626571655)\n",
      "('XDD', 0.8713074326515198)\n",
      "('昭銘', 0.8708035349845886)\n",
      "('國家隊', 0.8691540360450745)\n",
      "('哪裡', 0.8689779043197632)\n",
      "('有人', 0.8666054606437683)\n",
      "('當', 0.8665590286254883)\n",
      "('講', 0.8660700917243958)\n",
      "('球迷', 0.8652830123901367)\n",
      "('人家', 0.8645651340484619)\n",
      "('這種', 0.864547848701477)\n",
      "('那邊', 0.8630650043487549)\n",
      "('選手', 0.8628682494163513)\n",
      "('吹', 0.8615884780883789)\n",
      "('那', 0.8614248633384705)\n",
      "('認真', 0.8612918853759766)\n",
      "('算', 0.8605971336364746)\n",
      "('搞', 0.8591052293777466)\n",
      "('現場', 0.8589720129966736)\n",
      "('看球', 0.8563144207000732)\n",
      "('畫面', 0.8559941649436951)\n",
      "('喊', 0.8515591621398926)\n",
      "('玩', 0.8489598631858826)\n",
      "('企聯', 0.8485584855079651)\n",
      "('國外', 0.8482627272605896)\n",
      "('隊友', 0.8482041954994202)\n",
      "('做', 0.8475308418273926)\n",
      "('兩次', 0.8471142649650574)\n",
      "('之後', 0.8468199372291565)\n",
      "('所以', 0.846376895904541)\n",
      "('而已', 0.8463481068611145)\n",
      "('判', 0.846088707447052)\n",
      "('對方', 0.8454220294952393)\n",
      "('聽', 0.8423911929130554)\n",
      "('英文', 0.8419588208198547)\n",
      "('當然', 0.8415738940238953)\n",
      "('前面', 0.8413705229759216)\n",
      "('不能', 0.8406715989112854)\n",
      "('拍', 0.8402939438819885)\n",
      "('直接', 0.8401434421539307)\n",
      "('找', 0.8397246599197388)\n",
      "('頭髮', 0.8391978740692139)\n",
      "('棒球', 0.8390986919403076)\n",
      "('帶', 0.8385851383209229)\n",
      "('觀眾', 0.8384941220283508)\n",
      "('後面', 0.8384175300598145)\n",
      "('只能', 0.8381748795509338)\n",
      "('接', 0.8378146290779114)\n",
      "('規則', 0.8375478386878967)\n",
      "('就要', 0.8374495506286621)\n",
      "('先發', 0.8354486227035522)\n",
      "('過', 0.8347898125648499)\n",
      "('然後', 0.8346625566482544)\n",
      "('自我', 0.8341606259346008)\n",
      "('怎樣', 0.8340325355529785)\n",
      "('幹嘛', 0.8337574005126953)\n",
      "('嘴', 0.833668053150177)\n",
      "('分數', 0.8324267864227295)\n",
      "('換人', 0.8319646716117859)\n",
      "('踩', 0.8317985534667969)\n",
      "('公正', 0.8316290974617004)\n",
      "('場上', 0.8316100835800171)\n",
      "('有沒有', 0.8315436244010925)\n",
      "('日本', 0.8313630223274231)\n",
      "('呢', 0.8293319940567017)\n",
      "('落地', 0.8276916146278381)\n",
      "('注意', 0.8275803327560425)\n",
      "('籃球', 0.8271173238754272)\n",
      "('根本', 0.826761782169342)\n",
      "('進場', 0.8267286419868469)\n",
      "('賽制', 0.8266962170600891)\n",
      "('本來', 0.8261265158653259)\n",
      "('影響', 0.8254359364509583)\n",
      "('位置', 0.824859619140625)\n",
      "('過去', 0.8240413069725037)\n"
     ]
    }
   ],
   "source": [
    "keyword = '裁判'\n",
    "\n",
    "res = model.wv.similar_by_word(keyword, topn = 100)\n",
    "for item in res:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_emoji = ['☀', '⚡', '🌚', '🌝', '🌞', '🐳', '🐋', '🍊', '🏎', '🔆', '🐯', '🦁']\n",
    "cheer_emoji = ['⛽', '🏐', '💓', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '🏆', '👑', '🎉', '🎊', '❤', '🦾', '💯', '🙏', '💪', '🔥', '😍', '🤩']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/shenchingfeng/Desktop/Topic Modeling/team_player/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob('./*.txt')\n",
    "\n",
    "player_name = []\n",
    "for t in file:\n",
    "    with open(t, 'r') as f:\n",
    "        names = [name.strip() for name in f.readlines()]\n",
    "        player_name.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/shenchingfeng/Desktop/Topic Modeling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_file = './team.txt'\n",
    "team = []\n",
    "with open(team_file, 'r') as f:\n",
    "    team_name = [t.strip() for t in f.readlines()]\n",
    "\n",
    "team_name.append('球隊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheer_words = ['加油', '恭喜', '最棒', '穩住', '撐住', '一鼓作氣', '棒棒', '衝啊', '再接再厲', '追分', '沒事', '別氣餒', '不要氣餒', '保持', '穩穩', '咬住', '穩贏', 'GOGOGO', '無敵', '777', '666', '緊張', '爽', '啊啊', '刺激', '哇靠', '喔喔', '精彩', '好球', '水啦', '水唷', '來了', 'OMG', '厲害', '很棒', '哈哈', 'ha', 'HA', 'Ha', '激動', '狂', '神', '太強', '帥', '送啦', '好看', '讚', '好猛', '可惜', '耶', '超鬼', 'lucky', 'Lucky', 'Wow', 'wow', 'WOW', 'yey', 'yes', 'YEY', 'Yey', 'Yes', 'YES', '+u', '贏球', '水喔', '氣勢', '猛', '感動', '贏']\n",
    "tactic_words = ['攔網', '快攻', '進攻', '后排', '後排', '前排', '發球', '攻擊', '防守', '接發', '救', '接球', '到位', '接發球', '發球', '攔', '舉球', '一傳', '角度', '主攻', '二傳', '輪轉', 'block', 'IN', 'in', 'In', '界內', '封網', '失誤', '觸球', '暫停', '換人', '落地', '得分', '舉', '界外', 'outside', '局末', 'ACE', 'deuce', 'ace', 'Deuce', 'deu', '吊球', '短球', 'touch', '平手', '觸網', '自由', '連擊', '欄網']\n",
    "coach_words = ['連董', '小牛', '牛', '鄧公', '教練', '教练', '舟']\n",
    "stream_words = ['導播', '小編', '球評', '轉播', '講評', '聊天室', '畫面', '直播', '致平', '大康', '畫質', '邱老師', '電視', '邱老师', '000人', '000 人', '000', '千人', 'HOP SPORTS', '鏡頭', '攝影機', 'HOP Sports', '看球', '人數', '視角', '重播', '運鏡', '鏡位', '延遲', '祐哥']\n",
    "judge_words = ['裁判', '誤判', '挑戰', '規則', '主播', '吹', '鷹眼', '黃牌', '抗議', '判']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加油 \n",
    "def contains_cheer_emoji(text): ## 3890\n",
    "    return any(e in text for e in cheer_emoji)\n",
    "def contains_cheer_word(text):  ## 27990\n",
    "    return any(word in text for word in cheer_words)\n",
    "\n",
    "## 教練\n",
    "def contains_coach_word(text): ## 3211\n",
    "    return any(word in text for word in coach_words)\n",
    "\n",
    "## 戰術\n",
    "def contains_tactic_word(text): ## 14667\n",
    "    return any(word in text for word in tactic_words)\n",
    "\n",
    "## 球員\n",
    "def contains_player_name(text): ## 26534\n",
    "    return any(word in text for word in player_name)\n",
    "\n",
    "## 球隊\n",
    "def contains_team_name(text):  ## 18078\n",
    "    return any(word in text for word in team_name)\n",
    "def contains_team_emoji(text): ## 5007\n",
    "    return any(word in text for word in team_emoji)\n",
    "\n",
    "## 轉播\n",
    "def contains_stream_word(text):  ## 2684\n",
    "    return any(word in text for word in stream_words)\n",
    "\n",
    "## 裁判\n",
    "def contains_judge_word(text):  ## 2746\n",
    "    return any(word in text for word in judge_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['text'].apply(contains_coach_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows not meeting any condition: 46959\n"
     ]
    }
   ],
   "source": [
    "no_condition_rows = df[\n",
    "    (~df['text'].apply(contains_cheer_emoji)) &\n",
    "    (~df['text'].apply(contains_cheer_word)) &\n",
    "    (~df['text'].apply(contains_coach_word)) &\n",
    "    (~df['text'].apply(contains_tactic_word)) &\n",
    "    (~df['text'].apply(contains_team_emoji)) &\n",
    "    (~df['text'].apply(contains_player_name)) &\n",
    "    (~df['text'].apply(contains_team_name)) & \n",
    "    (~df['text'].apply(contains_stream_word)) & \n",
    "    (~df['text'].apply(contains_judge_word))\n",
    "]\n",
    "\n",
    "count_no_condition_rows = len(no_condition_rows)\n",
    "\n",
    "print(f\"Number of rows not meeting any condition: {count_no_condition_rows}\") ## 46959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比賽 / 加油 / 轉播 / 其他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1['比賽'] = ''\n",
    "df_h1['加油'] = ''\n",
    "df_h1['轉播'] = ''\n",
    "df_h1['球迷的聊天'] = ''\n",
    "\n",
    "for i, row in df_h1.iterrows():\n",
    "\n",
    "    if contains_cheer_emoji(row['text']) or contains_cheer_word(row['text']):\n",
    "        df_h1.at[i, '加油'] = 1\n",
    "    else:\n",
    "        df_h1.at[i, '加油'] = 0\n",
    "    \n",
    "    if contains_coach_word(row['text']) or contains_player_name(row['text']) or contains_tactic_word(row['text']) or contains_team_name(row['text']) or contains_team_emoji(row['text']) or contains_judge_word(row['text']):\n",
    "        df_h1.at[i, '比賽'] = 1\n",
    "    else:\n",
    "        df_h1.at[i, '比賽'] = 0\n",
    "\n",
    "    if contains_stream_word(row['text']):\n",
    "        df_h1.at[i, '轉播'] = 1\n",
    "    else:\n",
    "        df_h1.at[i, '轉播'] = 0\n",
    "    \n",
    "    if not (contains_cheer_emoji(row['text']) or contains_cheer_word(row['text']) or contains_coach_word(row['text']) or contains_player_name(row['text']) or contains_tactic_word(row['text']) or contains_team_name(row['text']) or contains_team_emoji(row['text']) or contains_judge_word(row['text']) or contains_stream_word(row['text'])):\n",
    "        df_h1.at[i, '球迷的聊天'] = 1\n",
    "    else:\n",
    "        df_h1.at[i, '球迷的聊天'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_h1[['比賽', '加油', '轉播', '球迷的聊天']]\n",
    "y = y.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('小温加油！美津濃加油🔥🔥', {'cats': {'比賽': 1, '加油': 1, '轉播': 0, '球迷的聊天': 0}})\n"
     ]
    }
   ],
   "source": [
    "dataset = list(zip(df_h1['text'],[{'cats': cats} for cats in y.values()]))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'curated_transformer' for language Chinese (zh). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, transformer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/shenchingfeng/Downloads/Topic Modeling/word2vec/df_1209/w2v_try.ipynb 儲存格 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shenchingfeng/Downloads/Topic%20Modeling/word2vec/df_1209/w2v_try.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# nlp = spacy.load('zh_core_web_trf')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shenchingfeng/Downloads/Topic%20Modeling/word2vec/df_1209/w2v_try.ipynb#Y144sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nlp \u001b[39m=\u001b[39m zh_core_web_trf\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/zh_core_web_trf/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    683\u001b[0m     data_path,\n\u001b[1;32m    684\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    685\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    686\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    687\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    688\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    689\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    690\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    537\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config, for_overrides\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    538\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[0;32m--> 539\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    542\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    543\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    544\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    545\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    547\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[1;32m    586\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 587\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[1;32m    588\u001b[0m     config,\n\u001b[1;32m    589\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    590\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    591\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    592\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    593\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[1;32m    594\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    595\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    596\u001b[0m )\n\u001b[1;32m    597\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1864\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     factory \u001b[39m=\u001b[39m pipe_cfg\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfactory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1862\u001b[0m     \u001b[39m# The pipe name (key in the config) here is the unique name\u001b[39;00m\n\u001b[1;32m   1863\u001b[0m     \u001b[39m# of the component, not necessarily the factory\u001b[39;00m\n\u001b[0;32m-> 1864\u001b[0m     nlp\u001b[39m.\u001b[39;49madd_pipe(\n\u001b[1;32m   1865\u001b[0m         factory,\n\u001b[1;32m   1866\u001b[0m         name\u001b[39m=\u001b[39;49mpipe_name,\n\u001b[1;32m   1867\u001b[0m         config\u001b[39m=\u001b[39;49mpipe_cfg,\n\u001b[1;32m   1868\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m   1869\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[1;32m   1870\u001b[0m     )\n\u001b[1;32m   1871\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1872\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m pipe_cfg\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:821\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    817\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[1;32m    818\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 821\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[1;32m    822\u001b[0m         factory_name,\n\u001b[1;32m    823\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    824\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    825\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[1;32m    826\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    827\u001b[0m     )\n\u001b[1;32m    828\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[1;32m    829\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:690\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[1;32m    683\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    684\u001b[0m         name\u001b[39m=\u001b[39mfactory_name,\n\u001b[1;32m    685\u001b[0m         opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m         lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[1;32m    689\u001b[0m     )\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    691\u001b[0m pipe_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n\u001b[1;32m    692\u001b[0m \u001b[39m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'curated_transformer' for language Chinese (zh). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, transformer"
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load('zh_core_web_trf')\n",
    "nlp = zh_core_web_trf.load()\n",
    "\n",
    "# config = Config().from_str(multi_label_cnn_config)\n",
    "\n",
    "# text_cat = nlp.add_pipe(\"textcat_multilabel\", config = config)\n",
    "\n",
    "# labels = [\"比賽\", \"加油\", \"轉播\", \"球迷的聊天\"]\n",
    "\n",
    "# text_cat.add_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for text, annotations in train_data:\n",
    "        doc = nlp(text)\n",
    "        example = spacy.training.Example.from_dict(doc, annotations)\n",
    "        text_cat.update([example])\n",
    "\n",
    "nlp.to_disk(\"multi_label_model_zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tvl.ctvba.org.tw/news'\n",
    "response = requests.get(url)\n",
    "news = {'URL': [], 'News': []}\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    news_links = soup.find_all('a', href = lambda href: href and \"/news-detail/\" in href)\n",
    "\n",
    "    for link in news_links:\n",
    "        news_url = 'https://tvl.ctvba.org.tw' + link['href']\n",
    "        news_response = requests.get(news_url)\n",
    "\n",
    "        if news_response.status_code == 200:\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            description_meta = news_soup.find('meta', {'name': 'description'})\n",
    "\n",
    "            if description_meta:\n",
    "                description_content = description_meta['content']\n",
    "\n",
    "                news['URL'].append(news_url)\n",
    "                news['News'].append(description_content)\n",
    "else:\n",
    "    print('Failed to retrieve the page. Status Code:', response.status_code)\n",
    "\n",
    "news = pd.DataFrame(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = news.drop_duplicates(subset = ['News']).reset_index(drop = True)\n",
    "# news['News'] = news['News'].str.replace('\\r\\n', '')\n",
    "# news['News'] = news['News'].str.replace('\\xa0', '')\n",
    "# news = news[:307]\n",
    "# news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_final = []\n",
    "\n",
    "# for index, row in news.iterrows():\n",
    "#     segments = tokenizer(row['News'])\n",
    "#     news_final.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = final + news_final\n",
    "# all_filtered = [sublist for sublist in all if len(sublist) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = [' '.join(rrr) for rrr in all_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(all_filtered, vector_size = 300, min_count = 3, workers = -1, sg = 1)\n",
    "# model.save(\"1209W2V_News_and_Chat.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('還糟', 0.279350221157074)\n",
      "('停車位', 0.2471543848514557)\n",
      "('waaaaahhhh', 0.244615837931633)\n",
      "('短期內', 0.24159204959869385)\n",
      "('分部', 0.22809460759162903)\n",
      "('達叔', 0.22177664935588837)\n",
      "('看不太到', 0.21698904037475586)\n",
      "('看西米', 0.21578575670719147)\n",
      "('看小姿', 0.21564465761184692)\n",
      "('非同小可', 0.21360906958580017)\n"
     ]
    }
   ],
   "source": [
    "# keyword = 'Carla'\n",
    "\n",
    "# res = word2vec_model.wv.similar_by_word(keyword, topn = 10)\n",
    "# for item in res:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_list = df_remove_emoji['text'].tolist()\n",
    "# news_list = news['News'].tolist()\n",
    "# documents = text_list + news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from gensim.models import Word2Vec\n",
    "# import numpy as np\n",
    "\n",
    "# documents = [str(doc) for doc in documents]\n",
    "\n",
    "# # Step 1: Tokenize using Jieba and compute TF-IDF vectors\n",
    "# jieba_documents = [' '.join(jieba.cut(doc)) for doc in documents]\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(jieba_documents)\n",
    "\n",
    "# # Step 2: Train Word2Vec model\n",
    "# word2vec_model = Word2Vec(sentences=[jieba.lcut(doc) for doc in documents], vector_size = 300, window = 5, min_count = 1, workers = -1)\n",
    "\n",
    "# # Step 3: Weight Word2Vec vectors with TF-IDF scores\n",
    "# weighted_word_vectors = []\n",
    "# for i, doc in enumerate(documents):\n",
    "#     tfidf_scores = {word: tfidf_matrix[i, tfidf_vectorizer.vocabulary_[word]] for word in jieba.lcut(doc) if word in tfidf_vectorizer.vocabulary_}\n",
    "#     weighted_vectors = [word2vec_model.wv[word] * tfidf_scores[word] for word in tfidf_scores]\n",
    "#     if weighted_vectors:\n",
    "#         weighted_doc_vector = np.mean(weighted_vectors, axis=0)\n",
    "#         weighted_word_vectors.append(weighted_doc_vector)\n",
    "#     else:\n",
    "#         weighted_word_vectors.append(np.zeros(word2vec_model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[0.9999997 0.6943898]\n",
      " [0.6943898 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# similarity_matrix = cosine_similarity(weighted_word_vectors[:2], weighted_word_vectors[:2])\n",
    "# print(\"Cosine Similarity Matrix:\")\n",
    "# print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
